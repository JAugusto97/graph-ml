{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Recap </b>\n",
    "\n",
    "Previously we discussed traditional feature based methods for graph ML, where given a graph G = (V,E) we wanted to extract node, link and graph-level features and learn a model (svm, logistic regressor, etc.) that maps features to labels.\n",
    "\n",
    "Input Graph -> Feature Engineering -> Learning Algorithm -> Prediction\n",
    "\n",
    "<b> Now </b>\n",
    "\n",
    "With <b>Representation Learning</b> we automatically learn the features, without needing to manually construct the features as we did before (feature engineering). \n",
    "\n",
    "<b> Goal </b>\n",
    "\n",
    "Efficient task-independent feature learning for machine learning with graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> map nodes into an embedding space\n",
    "\n",
    "* Similarity of embeddings between nodes indicates their similarity in the network.\n",
    "* Encode network information\n",
    "* Potentially used for many downstream prediction tasks such as node classification, link prediction, graph classification, anomalous node detection, clustering, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Goal:</b> Find an encoder function such that, when nodes u and v are encoded to the embedding space, the following assumption is true:\n",
    "the similarity between nodes u and v in the network is close to the similarity of the embedding representation of u and v in embedding space.\n",
    "\n",
    "similarity(u,v) ~ z_v.T @ z_u\n",
    "\n",
    "1. Encoder maps nodes to embeddings: Encoder(u) -> z_u.\n",
    "2. Define a similarity function for the original network.\n",
    "3. Decoder maps from embeddings to the embedding similarity score: dot preduct.\n",
    "4. Optimize the parameters such that similarity(u,v) ~ decoder(encoder(u), encoder(v)).\n",
    "\n",
    "This is an <b>unsupervised</b> way of learning node embeddings so we do not use labels or features. Also, the embeddings are task independent, because they are not trained for a specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Shallow\" Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplest encoding approach: encoder is just an embedding-lookup. Each node is assigned a unique embedding vector, so we optimize the embedding of each node using one of many methods: DeepWalk, node2vec. Each optimization method defines its own similarity measure.\n",
    "\n",
    "Encoder(v) = z_v = Z . v\n",
    "\n",
    "Z in R ^ (d x |V|) - Z is a matrix with each column represents a node embedding (what we learn/optimize).\n",
    "\n",
    "v in I ^ |V| - v is an indicator vector, all zeroes except a onde in column indicating node Vb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions -- Z: (128, 100) v: (100,) z: (128,)\n",
      "z is equal to the first column of Z: True\n"
     ]
    }
   ],
   "source": [
    "# 100 nodes with embedding size of 128\n",
    "Z = np.random.randn(128, 100) #  this needs to be optimized somehow. In this example its a random matrix\n",
    "\n",
    "# 100 entries for the indicator vetor, one for each node\n",
    "v = np.zeros(100)\n",
    "v[0] = 1\n",
    "\n",
    "z = Z @ v\n",
    "\n",
    "print(\"Dimensions -- Z:\", Z.shape, \"v:\", v.shape, \"z:\", z.shape)\n",
    "print(\"z is equal to the first column of Z:\", (Z @ v == Z[:,0]).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random-Walk Approaches for Node Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Random Walk</b>\n",
    "\n",
    "Given a graph G = (V, E) and a starting point v in V, we select a neighbor u in N(v) at random and move to u. Then we repeat this process for a random node z in N(u) and so on, creating a random sequence of nodes visited from a starting point.\n",
    "\n",
    "Note: the same node can be visited multiple times in a random walk.\n",
    "\n",
    "Provides:\n",
    "\n",
    "1. Expressivity: flexible stochastic defintion of node similarity that incorporates both local and higher-order neighborhood information. Intuition: if random walk starting from node u visits v with high probability, u and v are similar (high-order multi-hop information).\n",
    "2. Efficiency: do not need to consider all node pairs when training. Only need to consider pairs that co-occur on random walks. \n",
    "\n",
    "<b>Random-Walk Embeddings</b>\n",
    "\n",
    "Goal: learn embeddings Z such that z_u.T @ z_v ~ probability that u and v co-occur on a random walk over the graph. In other words, nodes that appear togheter frequently in many random walks should have similar embedding representations.\n",
    "\n",
    "1. Estimate the probability of visiting node v on a random walk starting from node u: P(v|u)\n",
    "2. Optimize embeddings to encode these random walk statistics: cosine theta between z_u and z_v is proportional to P(v|u). This step is described in details below.\n",
    "\n",
    "<b>Random Walk Optimization</b>\n",
    "\n",
    "1. Run short fixed-length random walks starting from each node u in the graph.\n",
    "2. For each node u collect N_r(u), the multiset of nodes visited on random walks starting from u. N_r(u) can repeat elements since nodes can be visited multiple times on random walks.\n",
    "3. Optimize embeddings according to: given node u, predict its neighbors N_r(u) using z_u.\n",
    "\n",
    "max (for each u in V -> log P(N_r(u)|z_u)) -- maximum likelihood objective.\n",
    "\n",
    "Equivalently, we can write:\n",
    "\n",
    "> L = for each u in V and for each v in N_r(u) -> -log(P(v|z_u))\n",
    "\n",
    "Optimize embeddings z_u to maximize the likelihood of random walk co-occurrences.\n",
    "\n",
    "P(v|z_u) is the softmax of the dot product between z_u and z_v. P(v|z_u) = softmax(z_u.T @ z_v). \\*\n",
    "\n",
    "Finally: find embeddings z_u that minimize L by applying stochastic gradient descent:\n",
    "\n",
    "* initilize z_u randomly for each u in V.\n",
    "* Iterate until converge:\n",
    "* For all nodes u, compute the derivative of z_u w.r.t L.\n",
    "* for all nodes u, make a step towards the diretion of the derivative: z_u <- z_u - learning_rate * derivative of z_u w.r.t L.\n",
    "\n",
    "<b> * Consideration: Negative Sampling </b>\n",
    "\n",
    "Softmax is defined as exp(z_u.T @ z_v)/(sum for each n in V -> exp(z_u.T @ z_n)). This is O(|V|^2) because of the denominator that iterates over every node in V. To reduce this computation, we use negative sampling.\n",
    "\n",
    "Negative sampling is a technique that samples k nodes at random (usually k = 5 up to k = 20 ) instead of using all nodes in V. The k nodes are not sellected with uniform probability, but with a biased probability. The highest the degree of a node, the bigger the probability of it being sampled.\n",
    "\n",
    "softmax(z_u.T @ z_v) ~ log(sigmoid(z_u.T @ z_v)) - sum from i to k -> log (sigmoid(z_u.T @ z_i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('community-detection')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55f33903720bb64860315298d5bed7e4c263e079c313d6ba1c9fac36a660d77c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
